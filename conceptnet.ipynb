{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7464dca2-179e-4f62-9a25-af8b72eff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import spacy\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import lemminflect\n",
    "import time\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAI_api_key = \"ENTER YOUR API KEY HERE\"\n",
    "openAI_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Download necessary data for WordNetLemmatizer if we haven't already\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"test\") # Just a test to trigger lookup error if not downloaded\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4') # Open Multilingual Wordnet, often needed for full WordNet functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b9ade3b3-2241-4953-b32e-7fb3bfefbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_word_types = [\n",
    "    \"ADJ\",\t\n",
    "    \"ADV\",\n",
    "    \"INTJ\",\t\n",
    "    \"NOUN\",\n",
    "    \"PROPN\",\t\n",
    "    \"VERB\"\n",
    "]\n",
    "\n",
    "NOUN_RELS = [\"RelatedTo\", \"CapableOf\", \"IsA\", \"UsedFor\", \"AtLocation\", \"HasPrerequisite\", \"HasProperty\", \"ReceivesAction\", \"CreatedBy\", \"Causes\", \"HasA\", \"MadeOf\"]\n",
    "NOUN_REV_RELS = [\"AtLocation\", \"IsA\", \"PartOf\"]\n",
    "VERB_RELS = [\"MannerOf\", \"HasSubevent\", \"MotivatedByGoal\", \"IsA\", \"HasFirstSubevent\", \"HasLastSubevent\"]\n",
    "VERB_REV_RELS = [\"CapableOf\", \"MannerOf\", \"CausesDesire\", \"CreatedBy\", \"UsedFor\", \"ReceivesAction\"]\n",
    "ADJ_RELS = [\"SimilarTo\", \"Antonym\", \"RelatedTo\", \"HasProperty\"] # Added for completeness\n",
    "\n",
    "\n",
    "stopwords = set([\n",
    "    'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than', 'get', 'put',\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "df9f348a-5bd6-4c0e-adf7-efd80347f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_word_lemma(word : str, pos_hint : str = None) -> str:\n",
    "    \"\"\"\n",
    "    Gets the lemma of a single word without sentence context using NLTK's WordNetLemmatizer.\n",
    "    pos_hint can be 'n' (noun), 'v' (verb), 'a' (adjective), 'r' (adverb).\n",
    "    Defaults to 'n' if no hint is given.\n",
    "    \"\"\"\n",
    "    return lemmatizer.lemmatize(word.lower(), pos=pos_hint) if pos_hint else lemmatizer.lemmatize(word.lower())\n",
    "\n",
    "def get_all_possible_lemmas(word: str) -> list[str]:\n",
    "    res = []\n",
    "    for hint in [\"v\", \"n\", \"a\", \"r\", \"s\"]:\n",
    "        res.append(get_word_lemma(word, hint))\n",
    "    return res\n",
    "\n",
    "def get_all_inflections(word : str) -> list[str]:\n",
    "    inflections = lemminflect.getAllInflections(word)\n",
    "    res = []\n",
    "    for _, i in inflections.items():\n",
    "        res += list(i)\n",
    "    return res\n",
    "\n",
    "def get_word_classes(word : str) -> set:\n",
    "    pos_set = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        #print(f\"Synset: {synset}\")\n",
    "        if synset.name().split('.')[0] != word:\n",
    "            continue  # Only consider exact matches\n",
    "        if synset.pos() == 'n':\n",
    "            pos_set.add('noun')\n",
    "        elif synset.pos() == 'v':\n",
    "            pos_set.add('verb')\n",
    "        elif synset.pos() == 's':\n",
    "            pos_set.add('adj')\n",
    "    return pos_set\n",
    "\n",
    "def remove_stopwords(words : str) -> str:\n",
    "    word_list = words.split()\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "    \n",
    "\n",
    "class Codemaster:\n",
    "    def __init__(self, our_words : list[str], enemy_words : list[str], civilian_words : list[str], assassin_word : str, my_team : str):\n",
    "        self.our_words = our_words\n",
    "        self.enemy_words = enemy_words\n",
    "        self.civilian_words = civilian_words\n",
    "        self.assassin_word = assassin_word\n",
    "        self.my_team = my_team\n",
    "\n",
    "        self.nlp = spacy.load(\"en_core_web_trf\")\n",
    "        \n",
    "        self.conceptnet_client = httpx.AsyncClient(http2=True)\n",
    "        self.openai_client = OpenAI(api_key=openAI_api_key)\n",
    "\n",
    "        self.lemmas = set()\n",
    "        all_words = our_words + enemy_words + civilian_words + [assassin_word]\n",
    "        for word in all_words:\n",
    "            self.lemmas.update(word)\n",
    "            self.lemmas.update(get_all_inflections(word))\n",
    "            self.lemmas.update(get_all_possible_lemmas(word))\n",
    "\n",
    "    async def fetch_conceptnet(self, url : str) -> dict:\n",
    "        r = await self.conceptnet_client.get(url, follow_redirects=True)\n",
    "        return r.json()\n",
    "    \n",
    "    def _filter(self, words: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        return the filtered list of words. Each word must NOT appear as a lemma or an inflection of any words on the board (which is given by self.lemmas).\n",
    "        \"\"\"\n",
    "        doc = self.nlp(words)\n",
    "        l = list(filter(lambda token: (token.pos_ in allowed_word_types) and (token.lemma_ not in self.lemmas) and (token.text not in self.lemmas), doc)) \n",
    "        # l is a list of tokens, convert them to string to return\n",
    "        return [token.text for token in l]\n",
    "    \n",
    "    def process_edge(self, edge : dict, target_word):\n",
    "        \"\"\"\n",
    "        for a given target word, and a given edge it has on ConceptNet, process the node this edge points to:\n",
    "            + First, the node has to be in English.\n",
    "            + Second, the node may be multi-word, so we need to process each individual word in it. \n",
    "            Keep the open class words (adjective, nouns, verbs, etc) as specified in the universal POS tags: https://universaldependencies.org/u/pos/\n",
    "            + Then, we need to make sure that each word follows the rules of the game (no subword embedding, etc).\n",
    "        The `self.filter` function performs the last 2 filters.\n",
    "        \"\"\"\n",
    "\n",
    "        # find whether our target word is at the start node or end node of this edge\n",
    "        start_node = edge[\"start\"]\n",
    "        end_node = edge[\"end\"]\n",
    "\n",
    "        start_node_label = remove_stopwords(start_node[\"label\"])\n",
    "        end_node_label = remove_stopwords(end_node[\"label\"])\n",
    "        \n",
    "        if start_node_label == target_word:\n",
    "            if end_node[\"language\"] != \"en\": return []\n",
    "            label_words = self._filter(end_node_label)\n",
    "        else:\n",
    "            if start_node[\"language\"] != \"en\": return []\n",
    "            label_words = self._filter(start_node_label)\n",
    "        return label_words\n",
    "\n",
    "    async def _fetch_relations(self, target_word: str, rel_list: list, is_rev: bool = False) -> set:\n",
    "        \"\"\"Generic function to fetch and process edges for a list of relations.\"\"\"\n",
    "        clues = set()\n",
    "        tasks = []\n",
    "        for rel in rel_list:\n",
    "            node_param = \"end\" if is_rev else \"start\"\n",
    "            url = f\"http://api.conceptnet.io/query?{node_param}=/c/en/{target_word}&rel=/r/{rel}&limit=10\"\n",
    "            tasks.append(self.conceptnet_client.get(url, follow_redirects=True))\n",
    "\n",
    "        responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        for i, res in enumerate(responses):\n",
    "            if isinstance(res, Exception):\n",
    "                # print(f\"Warning: Request for {target_word} with relation {rel_list[i]} failed: {res}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                api_res = res.json()\n",
    "                relation_name = rel_list[i]\n",
    "                for edge in api_res.get(\"edges\", []):\n",
    "                    processed_clues = self._process_edge(edge, target_word)\n",
    "                    for clue in processed_clues:\n",
    "                        clues.add((clue, relation_name))\n",
    "            except (ValueError, KeyError) as e:\n",
    "                # print(f\"Warning: Could not parse JSON for {target_word} with relation {rel_list[i]}: {e}\")\n",
    "                continue\n",
    "        return clues\n",
    "\n",
    "    \n",
    "    async def _fetch_clues_for_word(self, word: str, specific_rels: list = None) -> dict:\n",
    "        \"\"\"Fetches all potential clues for a single word, categorized by POS.\"\"\"\n",
    "        clues_by_pos = {'noun': set(), 'verb': set(), 'adj': set()}\n",
    "        word_classes = get_word_classes(word)\n",
    "        if not word_classes: word_classes.add('noun') # Default to noun\n",
    "\n",
    "        for pos in word_classes:\n",
    "            rels, rev_rels = [], []\n",
    "            if specific_rels:\n",
    "                rels = specific_rels\n",
    "                rev_rels = specific_rels\n",
    "            elif pos == 'noun':\n",
    "                rels, rev_rels = NOUN_RELS, NOUN_REV_RELS\n",
    "            elif pos == 'verb':\n",
    "                rels, rev_rels = VERB_RELS, VERB_REV_RELS\n",
    "            elif pos == 'adj':\n",
    "                rels, rev_rels = ADJ_RELS, []\n",
    "\n",
    "            forward_clues, reverse_clues = await asyncio.gather(\n",
    "                self._fetch_relations(word, rels, is_rev=False),\n",
    "                self._fetch_relations(word, rev_rels, is_rev=True)\n",
    "            )\n",
    "            clues_by_pos[pos].update(forward_clues)\n",
    "            clues_by_pos[pos].update(reverse_clues)\n",
    "        return clues_by_pos\n",
    "\n",
    "    async def get_all_potential_clues(self):\n",
    "        \"\"\"\n",
    "        Create multiple tasks to asynchronously get clues for each of our target words.\n",
    "        \"\"\"\n",
    "        self.conceptnet_client = httpx.AsyncClient(http2=True)\n",
    "        tasks = []\n",
    "        for word in self.our_words:\n",
    "            tasks.append(asyncio.create_task(self.fetch_and_extract_clues_for_word(word)))\n",
    "        res = await asyncio.gather(*tasks)\n",
    "        print(len(res))\n",
    "        await self.conceptnet_client.aclose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e7b3a-6dae-4148-9c4d-477eee07fa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d681c1cb-7645-4c30-bd2b-dd1efed53212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Time taken: 4.604136228561401\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "blue_words = [\"dwarf\", \"foot\", \"moon\", \"star\", \"ghost\", \"beijing\", \"fighter\", \"roulette\", \"alps\"]\n",
    "#red_words = [\"club\", \"superhero\", \"mount\", \"bomb\", \"knife\", \"belt\", \"robot\", \"rock\", \"bar\", \"lab\"]\n",
    "red_words = [\"drive\"]\n",
    "civilian_words = [\"dead\"]\n",
    "assassin_word = \"agent\"\n",
    "master = Codemaster(red_words, blue_words, civilian_words, assassin_word, \"red\")\n",
    "candidates = await master.get_all_potential_clues()\n",
    "print(f\"Time taken: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c42b1b-0fd1-4cbf-ac43-bc666bd857e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff1d6b-b9c3-4653-a810-02cf7fbee9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c1284c55-349c-4c6b-b7d9-280b3f47ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driving driving\n"
     ]
    }
   ],
   "source": [
    "l = ['drive', 'dwarf', 'foot', 'moon', 'star', 'ghost', 'beijing', 'fighter', 'roulette', 'alp', 'dead', 'agent']\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(\"driving\")\n",
    "for token in doc:\n",
    "    if token.lemma_ not in l:\n",
    "        print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bc979719-7a26-4a1c-ba4b-8c4593054d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input = f\"\"\"The following are possible word classes: [ADV (adverb), ADJ (adjective), INTJ (interjection), NOUN (noun), PROPN (proper noun), VERB (verb)].\n",
    "    \n",
    "    Determine which of these classes the word 'table' belongs to based on its possible usages in English (and English only).\n",
    "    \n",
    "    At the end of your response, list the corresponding abbreviations (e.g., NOUN, VERB), separated by commas, on a single line.\n",
    "    \"\"\"\n",
    "\n",
    ").output_text.split(\"\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ba13767d-195b-44be-b4f7-e22740245d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN, VERB'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f66913-64d3-4787-abf9-26bf190df904",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openAI_api_key)\n",
    "system_prompt = \"\"\"\n",
    "You are an expert English linguistic. You\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input = f\"\"\"\n",
    "    The following are possible word classes: [ADV (adverb), ADJ (adjective), INTJ (interjection), NOUN (noun), PROPN (proper noun), VERB (verb)].\n",
    "    \n",
    "    Determine which of these classes the word 'table' belongs to based on its possible usages in English (and English only).\n",
    "    \n",
    "    At the end of your response, list the corresponding abbreviations (e.g., NOUN, VERB), separated by commas, on a single line.\n",
    "    \"\"\"\n",
    "\n",
    ").output_text.split(\"\\n\")[-1]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d74fc-8421-475d-bfdc-e17f16e51f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20aaf797-19af-48a0-b8d0-c42bb9c026e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: Synset('huge.s.01')\n",
      "huge can be used as: adj\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_pos(word):\n",
    "    pos_set = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        print(f\"Synset: {synset}\")\n",
    "        if synset.name().split('.')[0] != word:\n",
    "            continue  # Only consider exact matches\n",
    "        if synset.pos() == 'n':\n",
    "            pos_set.add('noun')\n",
    "        elif synset.pos() == 'v':\n",
    "            pos_set.add('verb')\n",
    "        elif synset.pos() == 's':\n",
    "            pos_set.add('adj')\n",
    "            \n",
    "    return pos_set\n",
    "\n",
    "# Example usage\n",
    "word = \"huge\"\n",
    "pos_tags = get_pos(word)\n",
    "print(f\"{word} can be used as: {', '.join(pos_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f343583-53a4-458a-813a-87cb2fbed3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'city France'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"a city in France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6b700ba-2015-4d53-8954-b97a3b6030cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Time taken: 8.19223427772522\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201eef7-b912-43d8-98ee-e048a18af85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
