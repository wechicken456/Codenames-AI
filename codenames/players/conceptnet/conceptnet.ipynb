{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7464dca2-179e-4f62-9a25-af8b72eff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import lemminflect\n",
    "import time\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"test\") # Just a test to trigger lookup error if not downloaded\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4') # Open Multilingual Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9ade3b3-2241-4953-b32e-7fb3bfefbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_word_types = [\n",
    "    \"ADJ\",\t\n",
    "    \"ADV\",\n",
    "    \"INTJ\",\t\n",
    "    \"NOUN\",\n",
    "    \"PROPN\",\t\n",
    "    \"VERB\"\n",
    "]\n",
    "\n",
    "NOUN_RELS = [\"CapableOf\", \"IsA\", \"UsedFor\", \"AtLocation\", \"HasPrerequisite\", \"HasProperty\", \"ReceivesAction\", \"CreatedBy\", \"Causes\", \"HasA\", \"MadeOf\"]\n",
    "NOUN_REV_RELS = [\"AtLocation\", \"IsA\", \"PartOf\"]\n",
    "VERB_RELS = [\"MannerOf\", \"HasSubevent\", \"MotivatedByGoal\", \"IsA\", \"HasFirstSubevent\", \"HasLastSubevent\"]\n",
    "VERB_REV_RELS = [\"CapableOf\", \"MannerOf\", \"CausesDesire\", \"UsedFor\", \"CreatedBy\", \"ReceivesAction\", \"HasFirstSubevent\"]\n",
    "ADJ_RELS = [\"SimilarTo\", \"RelatedTo\", \"HasProperty\"] \n",
    "\n",
    "\n",
    "stopwords = set([\n",
    "    'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', \n",
    "    'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', \n",
    "    'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', \n",
    "    'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', \n",
    "    'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "    'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', \n",
    "    'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', \n",
    "    'here', 'than', 'get', 'put', 'thing', 'something', 'okay', 'stuff', 'things', 'thingy', 'whatever', 'whenever', 'whoever', 'wherever', 'alright', \n",
    "    'really', 'yeah', 'nope', 'yep', 'kinda', 'sorta', 'maybe', 'almost', 'like', 'just', 'anyway', 'somehow', 'someone', 'anyone', 'everyone', 'none',\n",
    "    'somebody', 'nobody', 'everybody', 'thingamajig', 'doohickey', 'meh', 'huh', 'yo', 'hi', 'hello', 'bye', 'goodbye', 'welcome', 'please', 'thanks', \n",
    "    'thank', 'ok', 'sure', 'nah', 'uh', 'um', 'oops', 'woo', 'yay', 'aw', 'ugh', 'eh', 'hmm', 'woah', 'uhh', 'lol', 'omg', 'idk', 'btw', 'fyi', 'brb', \n",
    "    'irl', 'tbh', 'really', 'literally', 'basically', 'seriously', 'clearly', 'surely', 'maybe', 'perhaps', 'somehow', 'sometime', 'eventually', \n",
    "    'already', 'soon', 'later', 'often', 'always', 'never', 'sometimes', 'anyway', 'just', 'even', 'simply', 'only', 'nearly', 'almost', 'barely', \n",
    "    'hardly', 'likely', 'possibly', 'probably', 'generally', 'typically', 'essentially', 'kindly', 'mostly', 'virtually', 'supposedly', 'approximately', \n",
    "    'theoretically', 'arguably'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df9f348a-5bd6-4c0e-adf7-efd80347f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_word_lemma(word : str, pos_hint : str = None) -> str:\n",
    "    \"\"\"\n",
    "    Gets the lemma of a single word without sentence context using NLTK's WordNetLemmatizer.\n",
    "    pos_hint can be 'n' (noun), 'v' (verb), 'a' (adjective), 'r' (adverb).\n",
    "    Defaults to 'n' if no hint is given.\n",
    "    \"\"\"\n",
    "    return lemmatizer.lemmatize(word.lower(), pos=pos_hint) if pos_hint else lemmatizer.lemmatize(word.lower())\n",
    "\n",
    "def get_all_possible_lemmas(word: str) -> list[str]:\n",
    "    res = []\n",
    "    for hint in [\"v\", \"n\", \"a\", \"r\", \"s\"]:\n",
    "        res.append(get_word_lemma(word, hint))\n",
    "    return res\n",
    "\n",
    "def get_all_inflections(word : str) -> list[str]:\n",
    "    inflections = lemminflect.getAllInflections(word)\n",
    "    res = []\n",
    "    for _, i in inflections.items():\n",
    "        res += list(i)\n",
    "    return res\n",
    "\n",
    "def get_word_classes(word : str) -> set:\n",
    "    pos_set = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        #print(f\"Synset: {synset}\")\n",
    "        if synset.name().split('.')[0] != word:\n",
    "            continue  # Only consider exact matches\n",
    "        if synset.pos() == 'n':\n",
    "            pos_set.add('noun')\n",
    "        elif synset.pos() == 'v':\n",
    "            pos_set.add('verb')\n",
    "        elif synset.pos() == 's':\n",
    "            pos_set.add('adj')\n",
    "\n",
    "    if len(pos_set) == 0: pos_set.add('noun') # default to noun if unidentified \n",
    "    return pos_set\n",
    "\n",
    "def remove_stopwords(words : str) -> str:\n",
    "    word_list = words.split()\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "    \n",
    "\n",
    "class ConceptNet:\n",
    "    def __init__(self, our_words : list[str], enemy_words : list[str], civilian_words : list[str], assassin_word : str):\n",
    "        self.our_words = [word.lower() for word in our_words]\n",
    "        self.enemy_words = [word.lower() for word in enemy_words]\n",
    "        self.civilian_words = [word.lower() for word in civilian_words]\n",
    "        self.assassin_word = assassin_word.lower()\n",
    "        self.all_board_words = set(our_words + enemy_words + civilian_words + [assassin_word])\n",
    "\n",
    "        self.clue_candidates = defaultdict(set)\n",
    "\n",
    "        self.strong_relations = {'IsA', 'PartOf', 'HasA', 'UsedFor', 'AtLocation', 'HasProperty', 'MannerOf', 'MotivatedByGoal'}\n",
    "        \n",
    "        # Spacy\n",
    "        self.nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "        self.conceptnet_client = httpx.AsyncClient(http2=True)\n",
    "\n",
    "        # All forms of board words\n",
    "        self.lemmas = set()\n",
    "        for word in self.all_board_words:\n",
    "            self.lemmas.update(word)\n",
    "            self.lemmas.update(get_all_inflections(word))\n",
    "            self.lemmas.update(get_all_possible_lemmas(word))\n",
    "\n",
    "        # Create graph from ConceptNet queries\n",
    "        self.graph = nx.DiGraph()\n",
    "\n",
    "\n",
    "    async def fetch_conceptnet(self, url : str) -> dict:\n",
    "        r = await self.conceptnet_client.get(url, follow_redirects=True)\n",
    "        return r.json()\n",
    "    \n",
    "    def _filter(self, words: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        return the filtered list of words. Each word must NOT appear as a lemma or an inflection of any words on the board (which is given by self.lemmas).\n",
    "        \"\"\"\n",
    "        def has_long_common_substring(a: str, b: str, min_len: int = 5) -> bool:\n",
    "            a, b = a.lower(), b.lower()\n",
    "            len_a, len_b = len(a), len(b)\n",
    "            for i in range(len_a):\n",
    "                for j in range(len_b):\n",
    "                    match_len = 0\n",
    "                    while (i + match_len < len_a) and (j + match_len < len_b) and a[i + match_len] == b[j + match_len]:\n",
    "                        match_len += 1\n",
    "                        if match_len >= min_len:\n",
    "                            return True\n",
    "            return False\n",
    "            \n",
    "        doc = nlp(words)\n",
    "        for token in doc:\n",
    "            for lemma in self.lemmas:\n",
    "                if (token.pos_ not in allowed_word_types) or (token.lemma_ == lemma) or (token.text == lemma) or (lemma in token.text) or (token.text in lemma) or (has_long_common_substring(token.text, lemma)):\n",
    "                    return False\n",
    "        \n",
    "        return [token.text for token in l]\n",
    "    \n",
    "    def _process_edge(self, edge : dict, target_word):\n",
    "        \"\"\"\n",
    "        for a given target word, and a given edge it has on ConceptNet, process the node this edge points to:\n",
    "            + First, the node has to be in English.\n",
    "            + Second, the node may be multi-word, so we need to process each individual word in it. \n",
    "            Keep the open class words (adjective, nouns, verbs, etc) as specified in the universal POS tags: https://universaldependencies.org/u/pos/\n",
    "            + Then, we need to make sure that each word follows the rules of the game (no subword embedding, etc).\n",
    "        The `self.filter` function performs the last 2 filters.\n",
    "        \"\"\"\n",
    "\n",
    "        # find whether our target word is at the start node or end node of this edge\n",
    "        start_node = edge[\"start\"]\n",
    "        end_node = edge[\"end\"]\n",
    "\n",
    "        start_node_label = remove_stopwords(start_node[\"label\"].lower())\n",
    "        end_node_label = remove_stopwords(end_node[\"label\"].lower())\n",
    "        \n",
    "        if target_word in start_node_label:\n",
    "            if end_node[\"language\"] != \"en\": return []\n",
    "            label_words = self._filter(end_node_label)\n",
    "        else:\n",
    "            if start_node[\"language\"] != \"en\": return []\n",
    "            label_words = self._filter(start_node_label)\n",
    "        return label_words\n",
    "\n",
    "    async def _fetch_relations(self, target_word: str, rel_list: list, limit : int, is_rev: bool = False) -> set:\n",
    "        \"\"\"Generic function to fetch and process edges for a list of relations. Create multiple parallel tasks to process these relations.\"\"\"\n",
    "        clues = set()\n",
    "        tasks = []\n",
    "        for rel in rel_list:\n",
    "            node_param = \"end\" if is_rev else \"start\"\n",
    "            url = f\"http://api.conceptnet.io/query?{node_param}=/c/en/{target_word}&rel=/r/{rel}&limit={limit}\"\n",
    "            tasks.append(self.conceptnet_client.get(url, follow_redirects=True))\n",
    "\n",
    "        responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        for i, res in enumerate(responses):\n",
    "            if isinstance(res, Exception):\n",
    "                # print(f\"Warning: Request for {target_word} with relation {rel_list[i]} failed: {res}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                api_res = res.json()\n",
    "                relation_name = rel_list[i]\n",
    "                for edge in api_res.get(\"edges\", []):\n",
    "                    processed_clues = self._process_edge(edge, target_word)\n",
    "                    for clue in processed_clues:\n",
    "                        clues.add((clue, relation_name, is_rev))\n",
    "            except (ValueError, KeyError) as e:\n",
    "                # print(f\"Warning: Could not parse JSON for {target_word} with relation {rel_list[i]}: {e}\")\n",
    "                continue\n",
    "        return clues\n",
    "\n",
    "    \n",
    "    async def _fetch_clues_for_word(self, word: str, specific_rels: list = None, limit : int = 8) -> dict:\n",
    "        \"\"\"\n",
    "        Fetches all potential clues for a single word. Depending on its POS (Part Of Speech), use the relation list accordingly.\n",
    "        If specific_rels is provided, use those relations instead.\n",
    "        \"\"\"\n",
    "        clues_by_pos = {'noun': set(), 'verb': set(), 'adj': set()}\n",
    "        word_classes = get_word_classes(word)\n",
    "\n",
    "        for pos in word_classes:\n",
    "            rels, rev_rels = [], []\n",
    "            if specific_rels:\n",
    "                rels = specific_rels\n",
    "                rev_rels = specific_rels\n",
    "            elif pos == 'noun':\n",
    "                rels, rev_rels = NOUN_RELS, NOUN_REV_RELS\n",
    "            elif pos == 'verb':\n",
    "                rels, rev_rels = VERB_RELS, VERB_REV_RELS\n",
    "            elif pos == 'adj':\n",
    "                rels, rev_rels = ADJ_RELS, []\n",
    "\n",
    "            forward_clues, reverse_clues = await asyncio.gather(\n",
    "                self._fetch_relations(word, rels, limit, is_rev=False),\n",
    "                self._fetch_relations(word, rev_rels, limit, is_rev=True)\n",
    "            )\n",
    "            clues_by_pos[pos].update(forward_clues)\n",
    "            clues_by_pos[pos].update(reverse_clues)\n",
    "        return clues_by_pos\n",
    "\n",
    "    async def build_clue_graph(self):\n",
    "        \"\"\"Builds the clue graph by querying ConceptNet up to 2 levels deep.\"\"\"     \n",
    "\n",
    "        total_start = time.time()\n",
    "        \n",
    "        # Add all board words first\n",
    "        for word in self.our_words: self.graph.add_node(word, type='our_word')\n",
    "        for word in self.enemy_words: self.graph.add_node(word, type='enemy_word')\n",
    "        for word in self.civilian_words: self.graph.add_node(word, type='civilian_word')\n",
    "        self.graph.add_node(self.assassin_word, type='assassin_word')\n",
    "\n",
    "        # Step 1: populate graph with depth-1 clues\n",
    "        print(\"\\n--- Building Clue Graph (Depth 1) ---\")\n",
    "        d1_tasks = []\n",
    "        for word in self.our_words:\n",
    "            d1_tasks.append(self._fetch_clues_for_word(word, limit=5))\n",
    "        \n",
    "        d1_results = await asyncio.gather(*d1_tasks)\n",
    "        \n",
    "        d1_clues_for_stage2 = []\n",
    "        for i, clues_by_pos in enumerate(d1_results):\n",
    "            source_word = self.our_words[i]\n",
    "            # Check if the word is a landmark\n",
    "            doc = self.nlp(source_word)\n",
    "            is_landmark = doc.ents and doc.ents[0].label_ in [\"GPE\", \"LOC\"]\n",
    "            \n",
    "            for pos, clue_tuples in clues_by_pos.items():\n",
    "                for clue, relation, is_rev in clue_tuples:\n",
    "                    if clue in self.all_board_words: continue\n",
    "                  \n",
    "                    if not self.graph.has_edge(source_word, clue):\n",
    "                        self.graph.add_edge(source_word, clue)\n",
    "                        self.clue_candidates[source_word].add(clue)\n",
    "                        if relation in self.strong_relations:\n",
    "                            d1_clues_for_stage2.append((source_word, clue, pos, relation))\n",
    "\n",
    "        print(f\"Graph after depth 1: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges.\")\n",
    "        print(f\"Time taken: {time.time() - total_start}\")\n",
    "\n",
    "        print(\"\\n--- Building Clue Graph (Depth 2) ---\")\n",
    "        start_2 = time.time()\n",
    "        d2_tasks = []\n",
    "        for _, d1_clue, _, rel in d1_clues_for_stage2:\n",
    "            d2_tasks.append(self._fetch_clues_for_word(d1_clue, specific_rels=[rel], limit=4))\n",
    "        \n",
    "        d2_results = await asyncio.gather(*d2_tasks)\n",
    "\n",
    "        for i, clues_by_pos in enumerate(d2_results):\n",
    "            source_word, d1_clue, pos, _ = d1_clues_for_stage2[i]\n",
    "            for _, clue_tuples in clues_by_pos.items():\n",
    "                for d2_clue, d2_relation, is_rev in clue_tuples:\n",
    "                    if d2_clue in self.all_board_words: continue\n",
    "                    \n",
    "                    if not self.graph.has_edge(d1_clue, d2_clue):\n",
    "                        self.clue_candidates[source_word].add(d2_clue)\n",
    "                        self.graph.add_edge(d1_clue, d2_clue)\n",
    "                        \n",
    "                        \n",
    "        print(f\"Finished stage 2.\\nTime taken: {time.time() - start_2}\")\n",
    "        print(f\"Graph after depth 2: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges.\")\n",
    "\n",
    "        print(\"\\n--- Finished building graph ---\")\n",
    "        print(f\"Total time taken: {time.time() - total_start}\")\n",
    "\n",
    "        \n",
    "    def find_candidate_clues(self, target_words: list[str]) -> set:\n",
    "        \"\"\"\n",
    "        Finds candidate clues by intersecting the pre-computed clue lists.\n",
    "        \"\"\"\n",
    "        if not target_words:\n",
    "            return set()\n",
    "\n",
    "        # Get the list of set of candidate clues for each target word\n",
    "        try:\n",
    "            candidate_sets = [set(self.clue_candidates[word.lower()]) for word in target_words]\n",
    "        except KeyError:\n",
    "            return set() # A target word might have no valid clues\n",
    "        \n",
    "        if not candidate_sets:\n",
    "            return set()\n",
    "\n",
    "        # Get intersection of all the sets\n",
    "        intersection = candidate_sets[0].intersection(*candidate_sets[1:])\n",
    "        return intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e7b3a-6dae-4148-9c4d-477eee07fa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3da6f-26a3-4c34-b909-f746d823a85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
