{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ab206a-2c0b-40a0-af84-0567b05d2be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pwnphofun/miniconda3/envs/codenames/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, load_from_disk\n",
    "import pickle\n",
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318f3584-f4d0-4334-9744-9e2859bc05e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting combination process for 2 chunks...\n",
      "--- Processing Chunk 1 ---\n",
      "--- Processing Chunk 2 ---\n",
      "\n",
      "[+] Data collection complete. All chunks processed.\n",
      "[*] Averaging embeddings and building final index...\n",
      "Building final Annoy tree with 668748 unique words...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# 2. Average them and build the new index\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embeddings_collection:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[43maverage_and_build_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_collection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[!] No data was found to process. Please check your file paths and NUM_CHUNKS.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36maverage_and_build_index\u001b[39m\u001b[34m(collection)\u001b[39m\n\u001b[32m     71\u001b[39m         item_idx += \u001b[32m1\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBuilding final Annoy tree with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_annoy_index.get_n_items()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m unique words...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mfinal_annoy_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Build with 100 trees\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Save the final combined index and word map\u001b[39;00m\n\u001b[32m     77\u001b[39m final_annoy_index.save(\u001b[33m'\u001b[39m\u001b[33mfinal_combined.ann\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "EMB_SIZE = 768\n",
    "NUM_CHUNKS = 2\n",
    "METRIC = 'angular\n",
    "\n",
    "# --- Main Combination Logic ---\n",
    "def combine_embeddings(num_chunks: int):\n",
    "    \"\"\"\n",
    "    Combines embedding data from multiple chunks into a single,\n",
    "    globally-averaged set of embeddings.\n",
    "    \"\"\"\n",
    "    # This dictionary will store the final combined data:\n",
    "    # { \"word\": (running_average_embedding, total_count) }\n",
    "    combined_data = {}\n",
    "\n",
    "    print(f\"[*] Starting combination process for {num_chunks} chunks...\")\n",
    "\n",
    "    for i in range(1, num_chunks + 1):\n",
    "        prefix = f\"chunk_{i}\"\n",
    "        \n",
    "        # Check if files for the chunk exist\n",
    "        if not os.path.exists(f\"{prefix}_embeddings.npy\"):\n",
    "            print(f\"[!] Warning: Data for chunk {i} not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- Processing Chunk {i} ---\")\n",
    "        \n",
    "        # Load data for the current chunk\n",
    "        chunk_embeddings = np.load(f\"{prefix}_embeddings.npy\")\n",
    "        chunk_counts = np.load(f\"{prefix}_counts.npy\")\n",
    "        with open(f\"{prefix}_word_to_idx.pkl\", \"rb\") as f:\n",
    "            chunk_word_to_idx = pickle.load(f)\n",
    "\n",
    "        # Iterate through each word in the current chunk\n",
    "        for word, idx in chunk_word_to_idx.items():\n",
    "            chunk_emb = chunk_embeddings[idx]\n",
    "            chunk_count = chunk_counts[idx]\n",
    "            \n",
    "            # If we've seen this word before, perform a weighted average\n",
    "            if word in combined_data:\n",
    "                total_emb, total_count = combined_data[word]\n",
    "                \n",
    "                # New average = ( (old_avg * old_count) + (new_avg * new_count) ) / (old_count + new_count)\n",
    "                new_total_count = total_count + chunk_count\n",
    "                \n",
    "                # np.average handles the weighted calculation automatically\n",
    "                new_avg_emb = np.average(\n",
    "                    [total_emb, chunk_emb], \n",
    "                    axis=0, \n",
    "                    weights=[total_count, chunk_count]\n",
    "                )\n",
    "                \n",
    "                combined_data[word] = (new_avg_emb, new_total_count)\n",
    "            \n",
    "            # If this is the first time seeing this word, just add it\n",
    "            else:\n",
    "                combined_data[word] = (chunk_emb, chunk_count)\n",
    "    \n",
    "    print(\"\\n[+] Combination complete. All chunks merged.\")\n",
    "    return combined_data\n",
    "\n",
    "def build_final_annoy_index(combined_data: dict):\n",
    "    \"\"\"\n",
    "    Builds and saves a final Annoy index from the combined data.\n",
    "    \"\"\"\n",
    "    print(\"[*] Building final Annoy index...\")\n",
    "    \n",
    "    t = AnnoyIndex(EMB_SIZE, metric=METRIC)\n",
    "    final_idx_to_word = {}\n",
    "    \n",
    "    for i, (word, (embedding, count)) in enumerate(combined_data.items()):\n",
    "        if len(embedding) == EMB_SIZE:\n",
    "            t.add_item(i, embedding)\n",
    "            final_idx_to_word[i] = word\n",
    "        \n",
    "    print(f\"Building Annoy tree with {t.get_n_items()} items...\")\n",
    "    t.build(100, n_jobs=-1) # Build with 100 trees\n",
    "\n",
    "    # Save the final index and the word mapping\n",
    "    t.save('final_annoy_index.ann')\n",
    "    np.save('final_idx_to_word.npy', final_idx_to_word)\n",
    "    \n",
    "    print(\"[+] Final Annoy index and word map saved successfully!\")\n",
    "    print(\"Files: 'final_annoy_index.ann' and 'final_idx_to_word.npy'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_data = combine_embeddings(NUM_CHUNKS)\n",
    "    if final_data:\n",
    "        build_final_annoy_index(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa65d5-f78d-46fa-8af9-70cb4eb05b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
